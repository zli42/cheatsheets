# Deep Learning

## 评估指标

$ cos = \frac{x \cdot y}{|x|\cdot|y|} $

$ precision = \frac{TP}{TP + FP} $

$ recall = \frac{TP}{TP + FN} $

$ F1 = \frac{2PR}{p+r} $

AUC

GAUC

## 优化策略

### Adam

* 优点：使用一阶动量和二阶动量，自适应学习率来加快收敛速度
* 缺点：后期学习率太低，影响收敛

## 激活函数

### sigmoid

$ \sigma(x) = \frac{1}{1 + e^{-x}} $

优点：

* 输出映射在 $ (0, 1) $ 之间，单调连续
* 容易求导

缺点：

* 落入饱和区梯度接近于 $0$，导致梯度消失
* 输出恒大于 $0$，使得后一层神经元的输入发生偏移，减缓梯度下降的收敛
* 计算中有幂运算，速度较慢

### tanh

$ tanh(x) = \frac{e^{x} - e^{-x}}{e^{x} + e^{-x}} $

优点：

* 输出以 $0$ 为中心，收敛速度快

缺点：

* 落入饱和区梯度接近于 $0$，导致梯度消失
* 计算中有幂运算，速度较慢

### ReLU

$ f(x) = \begin{cases}
0 & x \lt 0 \\
x & x \ge 0
\end{cases} $

优点：

* 计算见简单，收敛速度快
* 当 $ x \ge 0 $ 时，ReLU​ 的导数为常数，缓解梯度消失问题
* 当 $ x \lt 0 $ 时，ReLU​ 的梯度总是 $0$，提供了神经网络的稀疏表达能力

缺点：

* 输出不是以 0 为中心的
* 某些神经元可能永远不会被激活，导致相应参数永远不会被更新
* 不能避免梯度爆炸问题

### PReLU

$ f(x) = \begin{cases}
\alpha x & x \lt 0 \\
x & x \ge 0
\end{cases} $

优点：

* 自适应学习参数，收敛速度快
* 缓解梯度消失问题

缺点：

* 效果不一定比 ReLU 好
* 不能避免梯度爆炸问题

## 损失函数

$ softmax(x_i) = \frac{e^{x_i}}{\sum_{j=1}^k e^{x_j}} $

$ cross entropy = -y * \log{\hat{y}} $

## 正则化

* 数据增强
* L2 正则化（权重衰减）
* L1 正则化，稀疏，用于特征选择
* Dropout
* early stopping

## 参数初始化

### Xavier 初始化

保证输入和输出的方差基本一致，加快模型训练速度。

适用于线性激活函数

### Kaiming 初始化

适用于 ReLU 激活函数

## 归一化

### Min-max 归一化

$ \frac{x - mean(x)}{max(x) - min(x)} $

归一化到 $[0, 1]$，适合最大最小值明确不变且无极端异常值的情况，不改变数据分布

### Z-score 标准化

$ \frac{x - \mu}{\sigma} $ 

归一化后数据均值为 $0$， 标准差为 $1$，改变原数据分布

### Batch Normalization / Layer Normalization

随着网络的加深，在训练过程中神经元的输入值逐渐往非线性函数两端靠近，导致梯度消失，造成网络收敛变慢。

BN 层通过规范化把输入值拉回均值为 0 方差为 1 的标准正态分布，使输入落入非线性函数的敏感区域，这样输入的小变化就能导致梯度的大变化，加快收敛。同时，为了保持非线性，对规范化的输入又进行了 scale 和 shift（$y = scale*x + shift$），scale 和 shift 是通过训练学习到的，相当于将输入从激活函数的线性区域向非线性区域移动，保证了非线性表达能力。

训练时使用 mini-batch 的数据，推理时使用训练的全部数据。

BatchNorm 通过对 batch size 这个维度归一化来让分布稳定下来，LayerNorm 则是通过对 hidden size 这个维度归一。

一般来说，如果你的特征依赖于不同样本间的统计参数，那 BN 更有效。因为它抹杀了不同特征之间的大小关系，但是保留了不同样本间的大小关系。（CV 领域）
而在 NLP 领域，LN 就更加合适。因为它抹杀了不同样本间的大小关系，但是保留了一个样本内不同特征之间的大小关系。对于 NLP 或者序列任务来说，一条样本的不同特征，其实就是时序上字符取值的变化，样本内的特征关系是非常紧密的。

LN 针对单个训练样本进行，不依赖于其他数据，因此可以避免 BN 中受 mini-batch 数据分布影响的问题，可以用于 小mini-batch场景、动态网络场景和 RNN，特别是自然语言处理领域。此外，LN 不需要保存 mini-batch 的均值和方差，节省了额外的存储空间。
